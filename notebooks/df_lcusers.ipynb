{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46efe2f4-2b0a-4437-9461-df212b1624d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from datenspende.utils import query_ch_df, query_pg_df\n",
    "import datetime\n",
    "from datetime import date\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67678d07-7a4b-45e1-ac9b-0e8e19c456a4",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "215fef31-16d4-4725-8eb1-3fe13000dba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all epoch data on user\n",
    "def get_epoch(user_ids):\n",
    "    \n",
    "   \n",
    "    ft = tuple(user_ids)    \n",
    "    df = query_ch_df(\n",
    "            #\"\"\"DESCRIBE TABLE rocs.test_table\"\"\"\n",
    "        #\"\"\"SELECT * FROM rocs.vital_data_epoch WHERE vital_data_epoch.customer IN {formatter}\"\"\"\n",
    "        \"\"\"SELECT * FROM rocs.vital_data_epoch WHERE vital_data_epoch.customer IN {}\"\"\".format(ft) \n",
    "        #\"\"\"SELECT * FROM rocs.vital_data_epoch LIMIT 5000\"\"\"\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c506bef-1db3-470d-b2ff-8c52616e21c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get age\n",
    "def get_demo(user_ids):\n",
    "    \n",
    "    if isinstance(user_ids, int) or isinstance(user_ids, np.int64):\n",
    "        formatter = f'({user_ids})'\n",
    "    elif len(user_ids) == 1:\n",
    "        formatter = f'({user_ids[0]})'\n",
    "    else:\n",
    "        formatter = tuple(user_ids) \n",
    "    \n",
    "    \n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        user_id, salutation, birth_date, weight, height, creation_timestamp\n",
    "    FROM \n",
    "        rocs.datenspende.users\n",
    "    WHERE \n",
    "        users.user_id IN {formatter} \n",
    "   \n",
    "    \"\"\" \n",
    "\n",
    "    users = query_pg_df(query)\n",
    "    users.creation_timestamp = pd.to_datetime(users['creation_timestamp'],unit='ms') \n",
    "    users.creation_timestamp = users.creation_timestamp.dt.date\n",
    "    users['age'] = np.floor((2023 + 1 / 12) - users['birth_date'] + 2.5)\n",
    "\n",
    "    \n",
    "    \n",
    "    return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d90b940-108f-415b-a372-74c4a7864fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sex\n",
    "def get_info(user_ids):\n",
    "    \n",
    "    # Make sure that the IN-condition for the SQL query either takes the form '(userid)' in the case\n",
    "    # of a single requested user id or '(userid1, userid2, ..., useridN)' in the case of multiple\n",
    "    # requested user ids\n",
    "    if isinstance(user_ids, int) or isinstance(user_ids, np.int64):\n",
    "        formatter = f'({user_ids})'\n",
    "    elif len(user_ids) == 1:\n",
    "        formatter = f'({user_ids[0]})'\n",
    "    else:\n",
    "        formatter = tuple(user_ids)\n",
    "    \n",
    "    \n",
    "    \n",
    "    qu = f\"\"\"\n",
    "    select\n",
    "        \n",
    "        a.user_id,\n",
    "        a.created_at,\n",
    "        a.question,\n",
    "        a.element\n",
    "        \n",
    "    from \n",
    "        rocs.datenspende.answers a\n",
    "    where \n",
    "        a.user_id IN {formatter}\n",
    "    AND\n",
    "        a.question = 127\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    users = query_pg_df(qu)\n",
    "    users.created_at = pd.to_datetime(users['created_at'],unit='ms')\n",
    "    users.created_at = users.created_at.dt.date\n",
    "    \n",
    "    \n",
    "    return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40055f9b-c34c-4ca3-9baf-470b7252f765",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_types = pd.read_csv('epoch_value_types.csv')\n",
    "value_types = value_types.rename(columns={\"id\": \"type\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5b27c8a-49af-453f-ab5d-d72768601503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_s(st, end):\n",
    "    \n",
    "    if st == end:\n",
    "        end += pd.Timedelta(seconds=1)\n",
    "    return end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4535cec1-6b83-4632-b225-c71df003be91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_endv(st, end):\n",
    "    if end == pd.Timestamp('1970-01-01 00:00:00'):\n",
    "        end = st\n",
    "    return end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d183e57-3c62-4026-b4ca-5445a2071bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify epoch dataframe to get time info and bin in 15 minute intervals\n",
    "def modify_df(user_df):\n",
    "    \n",
    "    \n",
    "    if len(user_df['source'].unique()) > 1:\n",
    "        # if user has multiple devices, take device with max datapoints\n",
    "        d_len = []\n",
    "        for sour in ud['source'].unique():\n",
    "            d_len.append(len(user_df[user_df['source'] == sour]))    \n",
    "        user_df = user_df[user_df['source'] == user_df['source'].unique()[np.argmax(d_len)]].copy()\n",
    "            \n",
    "    else:\n",
    "        user_df = user_df.copy()\n",
    "        \n",
    "    user_df.startTimestamp = user_df.startTimestamp//1000\n",
    "    user_df.endTimestamp = user_df.endTimestamp//1000\n",
    "    user_df.startTimestamp = user_df.startTimestamp.apply(lambda x: datetime.datetime.fromtimestamp(x))\n",
    "    user_df.endTimestamp = user_df.endTimestamp.apply(lambda x: datetime.datetime.fromtimestamp(x))\n",
    "    user_df['date'] = user_df.startTimestamp.apply(lambda x: x.date())\n",
    "    \n",
    "    user_df = user_df.merge(value_types, how='left', on = 'type')\n",
    "    #user_df['entries'] = 1\n",
    "    \n",
    "    user_df['tdelta'] = user_df['endTimestamp'] - user_df['startTimestamp']\n",
    "    user_df['tdelta_min'] = user_df['tdelta'].apply(lambda x: x.total_seconds()//60)\n",
    "    user_df['tdelta_sec'] = user_df['tdelta'].apply(lambda x: x.total_seconds())\n",
    "    user_df['date'] = pd.to_datetime(user_df['date'])\n",
    "    \n",
    "    bins = list(range(0,97))\n",
    "    \n",
    "    \n",
    "    # if end timestamp = 1970-01-01 use start timestamp\n",
    "    # if tdeltasec = 0 add one second\n",
    "    \n",
    "    user_df['endTimestamp'] = user_df.apply(lambda x: clean_endv(x['startTimestamp'], x['endTimestamp']),axis=1)\n",
    "    user_df['endTimestamp'] = user_df.apply(lambda x: add_s(x['startTimestamp'], x['endTimestamp']),axis=1)\n",
    "    \n",
    "    user_df['Interval'] = user_df.apply(lambda x: pd.Interval(x['startTimestamp'],x['endTimestamp'],closed='right'), axis=1)   \n",
    "    \n",
    "    user_df['Time Bin 1'] = pd.cut((user_df.startTimestamp.dt.minute//15) + (user_df.startTimestamp.dt.hour * 4), bins,right=False)\n",
    "    user_df['Time Bin 2'] = pd.cut((user_df.endTimestamp.dt.minute//15) + (user_df.endTimestamp.dt.hour * 4), bins,right=False)\n",
    "    \n",
    "\n",
    "    \n",
    "    return user_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7839daea-0eb5-40d8-a53f-8f54bac078e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_overlaps(pdt, vt):\n",
    "    # per day find measurements which were recorded in overlapping time intervals \n",
    "    # combine them by summing up steps and adjusting the interval\n",
    "    # add new entry to df and drop old entries\n",
    "    pdt = pdt.copy()\n",
    "    pdt.index = np.arange(1, len(pdt) + 1)\n",
    "    drop_i = []\n",
    "    cps = []    \n",
    "    for iv in pdt['Interval']:\n",
    "        i = pdt[pdt['Interval'] == iv].index\n",
    "        ov = pdt[pdt['Time Bin 1'] == pdt['Time Bin 1'].loc[i].values[0]].copy()\n",
    "        ov['ov'] = ov['Interval'].apply(lambda x: x.overlaps(iv))\n",
    "        ov = ov[ov['ov']==True]\n",
    "        if len(ov) > 1:\n",
    "            i_min_s = ov[ov['startTimestamp'] == min(ov['startTimestamp'])].index[0]\n",
    "            i_max_e = ov[ov['endTimestamp'] == max(ov['endTimestamp'])].index[0]            \n",
    "            ts = (ov['endTimestamp'].loc[i_min_s] - ov['startTimestamp'].loc[i_min_s]).total_seconds()\n",
    "            \n",
    "            if vt == 'doubleValue':\n",
    "                td = (ov['startTimestamp'].loc[i_max_e]- ov['startTimestamp'].loc[i_min_s]).total_seconds()\n",
    "                f1 = td/ts\n",
    "                comb_val = (f1 * ov[vt].loc[i_min_s]) + ov[vt].loc[i_max_e]\n",
    "                comb_val = comb_val\n",
    "            elif vt == 'longValue':\n",
    "                te = (ov['endTimestamp'].loc[i_max_e]- ov['startTimestamp'].loc[i_max_e]).total_seconds()\n",
    "                tt = (ov['endTimestamp'].loc[i_max_e]- ov['startTimestamp'].loc[i_min_s]).total_seconds()\n",
    "                comb_val = np.average([ov[vt].loc[i_min_s],  ov[vt].loc[i_max_e]], weights=[ts/tt, te/tt])\n",
    "            elif vt == 'booleanValue':\n",
    "                \n",
    "                comb_val = ov[vt].loc[i_min_s]\n",
    "                \n",
    "            cp = ov.iloc[0].copy()\n",
    "            cp[vt] = comb_val\n",
    "            cp['ov'] = False\n",
    "            cp['startTimestamp'] = pd.Timestamp(ov['startTimestamp'].loc[i_min_s])\n",
    "            cp['endTimestamp'] = pd.Timestamp(ov['endTimestamp'].loc[i_max_e])\n",
    "            cp['Interval'] = pd.Interval(cp['startTimestamp'],cp['endTimestamp'],closed='neither')\n",
    "            cp['Time Bin 1'] = ov['Time Bin 1'].loc[i_min_s]\n",
    "            cp['Time Bin 2'] = ov['Time Bin 2'].loc[i_max_e]\n",
    "            #pdt = pdt.drop(ov.index,axis=0)\n",
    "            drop_i.append(ov.index)\n",
    "            cps.append(cp)\n",
    "            \n",
    "            #pdt.loc[len(pdt)+1] = cp\n",
    "            #print(i)\n",
    "           \n",
    "        pdt['ov'] = False\n",
    "    drop_i = list(set(sum([list(a) for a in drop_i], [])))\n",
    "    pdt = pdt.drop(drop_i,axis=0)\n",
    "    for c in cps:\n",
    "        pdt.loc[len(pdt)+1] = c\n",
    "    \n",
    "    return pdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a461507-8d83-4d61-83d8-d96908b337be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for measurements which fall into multiple time bins, calculate fraction of measured steps for each one of the time bins\n",
    "# for every unique time bin create a new row and drop the old one\n",
    "def multiple_bins(i_multiple_bins, dn, time_bins, bin_s, bin_e,vt):\n",
    "    cps = []\n",
    "    for mbi in i_multiple_bins:\n",
    "        mult_bin = dn.loc[mbi].copy()\n",
    "        dn = dn.drop(mbi,axis=0)\n",
    "        # get index of first time bin and last time bin of the multiple time bins to get all bins in between\n",
    "        first_bini = [t for t in range(len(time_bins)) if time_bins[t] == mult_bin['Time Bin 1']][0]\n",
    "        last_bini = [t for t in range(len(time_bins)) if time_bins[t] == mult_bin['Time Bin 2']][0]\n",
    "\n",
    "        t = mult_bin['endTimestamp']-mult_bin['startTimestamp'] # duration of the measurement\n",
    "        for b in range(first_bini, last_bini+1):\n",
    "            cp = mult_bin.copy()\n",
    "\n",
    "            if b == first_bini:\n",
    "                dur = datetime.datetime.combine(datetime.date.min, datetime.datetime.strptime(bin_e[b], '%H:%M:%S').time()) - datetime.datetime.combine(datetime.date.min,  mult_bin['startTimestamp'].time())\n",
    "                if vt == 'doubleValue':\n",
    "                    val = (dur/t) * mult_bin[vt]\n",
    "                elif (vt == 'longValue') or (vt == 'booleanValue'):\n",
    "                    val = mult_bin[vt]\n",
    "                cp['startTimestamp'] = mult_bin['startTimestamp']\n",
    "                cp['endTimestamp'] = cp['startTimestamp'] + dur\n",
    "\n",
    "            elif b == last_bini:\n",
    "                # in order to account for that second (when there are only two time bins the measurement falls into) time bin starts at the same time that first one ends, add one second to duration w factor a\n",
    "                if len(range(first_bini, last_bini+1)) > 2: \n",
    "                    a = datetime.timedelta(seconds=0) \n",
    "                else:\n",
    "                    a = datetime.timedelta(seconds=1)\n",
    "                dur = datetime.datetime.combine(datetime.date.min, mult_bin['endTimestamp'].time() ) - datetime.datetime.combine(datetime.date.min,  datetime.datetime.strptime(bin_s[b], '%H:%M:%S').time())\n",
    "                \n",
    "                if vt == 'doubleValue':\n",
    "                    val = ((dur+a)/t) * mult_bin[vt]\n",
    "                elif (vt == 'longValue') or (vt == 'booleanValue'):\n",
    "                    val = mult_bin[vt]\n",
    "                cp['startTimestamp'] = mult_bin['endTimestamp'] - dur\n",
    "                cp['endTimestamp'] = mult_bin['endTimestamp']\n",
    "            else:\n",
    "                # in order to account for that the n'th (when there are more than two time bins the measurement falls into) time bin starts at the same time that n-1 one ends, add one second to duration w factor a\n",
    "                if len(range(first_bini, last_bini+1)) < 2:\n",
    "                    a = datetime.timedelta(seconds=0)\n",
    "                else:\n",
    "                    a = datetime.timedelta(seconds=1)\n",
    "                bin_st = datetime.datetime.strptime(bin_s[b], '%H:%M:%S').time()\n",
    "                bin_et = datetime.datetime.strptime(bin_e[b], '%H:%M:%S').time() \n",
    "                dur = (datetime.datetime.combine(datetime.date.min, bin_et) - datetime.datetime.combine(datetime.date.min, bin_st ))#+1\n",
    "                if vt == 'doubleValue':\n",
    "                    al = ((dur+a)/t) * mult_bin[vt]\n",
    "                elif (vt == 'longValue') or (vt == 'booleanValue'):\n",
    "                    val = mult_bin[vt]\n",
    "                cp['startTimestamp'] = mult_bin['date'] + datetime.timedelta(hours=bin_st.hour, minutes=bin_st.minute, seconds=bin_st.second)\n",
    "                cp['endTimestamp'] = mult_bin['date'] + datetime.timedelta(hours=bin_et.hour, minutes=bin_et.minute, seconds=bin_et.second)\n",
    "            cp[vt] = val\n",
    "            cp['Time Bin 1'] = time_bins[b]\n",
    "            cp['Time Bin 2'] = time_bins[b]\n",
    "            cp['Interval'] = pd.Interval(cp['startTimestamp'],cp['endTimestamp'],closed='neither')\n",
    "            cps.append(cp)\n",
    "\n",
    "    # append new rows to dataframe\n",
    "    dn.index = np.arange(1, len(dn) + 1)\n",
    "    for c in cps:\n",
    "        dn.loc[len(dn)+1] = c\n",
    "    return dn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c220a401-696c-4ca1-a73c-bc93d54c6bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get starting and end times of hourly bins\n",
    "bin_s = []\n",
    "bin_e = []\n",
    "for h in range(24):\n",
    "    for m in range(4):\n",
    "    \n",
    "        time_string_s = '%02d:%02d:%02d' % (h,m * 15,0)\n",
    "        time_string_e = '%02d:%02d:%02d' % (h,(m * 15) + 14,59)\n",
    "        bin_s.append(time_string_s)\n",
    "        bin_e.append(time_string_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06d28738-a56a-483e-b98d-54419885fc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df for steps, hr and sleepstatebinary\n",
    "# one entry per day and time bin\n",
    "# combine with test info \n",
    "def get_df(udf):\n",
    "    \n",
    "    dates = []\n",
    "    tb = []\n",
    "\n",
    "    for d in udf['date'].unique():\n",
    "        for t in time_bins:\n",
    "            dates.append(d)\n",
    "            tb.append(t)\n",
    "            \n",
    "    df_comb = pd.DataFrame(data={'date':dates ,'Time Bin 1':tb})\n",
    "    \n",
    "    for v in [1000, 3000, 2000]:\n",
    "        if len(udf[udf['type'] == v]) > 0:\n",
    "            vt_i = udf[udf['type'] == v]['valueType'].iloc[0]\n",
    "            if vt_i == 0:\n",
    "                vt = 'doubleValue'\n",
    "            elif vt_i == 1:\n",
    "                vt = 'longValue'\n",
    "            elif vt_i == 2:\n",
    "                vt = 'booleanValue'\n",
    "\n",
    "            u_val_df = udf[udf['type'] == v][[vt, 'date', 'code', 'Time Bin 1', 'Time Bin 2', 'Interval','startTimestamp', 'endTimestamp']]\n",
    "            val_code = u_val_df['code'].iloc[0]\n",
    "\n",
    "            pdt_df = []\n",
    "            for date in u_val_df['date'].unique():\n",
    "                pdt_df.append(combine_overlaps(u_val_df[u_val_df['date']==date],vt)) \n",
    "            dn = pd.concat(pdt_df, axis=0)\n",
    "            # re-index\n",
    "            dn.index = np.arange(1, len(dn) + 1)\n",
    "\n",
    "\n",
    "            if vt_i == 0:\n",
    "                i_multiple_bins = dn[dn['Time Bin 1']!= dn['Time Bin 2']].index\n",
    "                dn = multiple_bins(i_multiple_bins, dn, time_bins, bin_s, bin_e,vt) \n",
    "                dng = dn[['date', 'Time Bin 1',vt]].groupby(['date', 'Time Bin 1']).sum( ).reset_index(level=[0,1])\n",
    "                dng = dng.rename(columns={vt:'steps'})\n",
    "\n",
    "                df_comb = pd.merge(df_comb,dng, how='outer', on=['date', 'Time Bin 1'])\n",
    "\n",
    "            elif vt_i == 1:\n",
    "\n",
    "                dn['s'] = dn['Interval'].apply(lambda x: x.length.total_seconds())\n",
    "                i_multiple_bins = dn[dn['Time Bin 1']!= dn['Time Bin 2']].index\n",
    "                i_multiple_bins = dn[(dn['Time Bin 1']!= dn['Time Bin 2'])&(dn['s'] != 60)].index\n",
    "                dn = multiple_bins(i_multiple_bins, dn, time_bins, bin_s, bin_e,vt) \n",
    "\n",
    "                dng = dn[['date', 'Time Bin 1',vt]].groupby(['date', 'Time Bin 1']).mean( ).reset_index(level=[0,1])\n",
    "                dng = dng.rename(columns = {vt:'hr'})         \n",
    "\n",
    "                #merge w df above\n",
    "                df_comb = pd.merge(df_comb, dng, how='outer', on=['date', 'Time Bin 1'])\n",
    "\n",
    "            elif vt_i == 2:\n",
    "                i_multiple_bins = dn[(dn['Time Bin 1']!= dn['Time Bin 2'])].index\n",
    "                dn = multiple_bins(i_multiple_bins, dn, time_bins, bin_s, bin_e,vt) \n",
    "                dng = dn[['date', 'Time Bin 1',vt]].groupby(['date', 'Time Bin 1']).mean( ).reset_index(level=[0,1])\n",
    "                dng = dng.rename(columns = {vt:'sleep'})    \n",
    "                #merge w df above\n",
    "                df_comb = pd.merge(df_comb, dng, how='outer', on=['date', 'Time Bin 1'])\n",
    "        elif len(udf[udf['type'] == v]) == 0:\n",
    "            \n",
    "            if v == 1000:\n",
    "                df_comb['steps'] = np.nan\n",
    "            elif v == 3000:\n",
    "                df_comb['hr'] = np.nan\n",
    "            elif v == 2000:\n",
    "                df_comb['sleep'] = np.nan\n",
    "                \n",
    "    return df_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a4cfd0c-e98a-4f05-bb5f-f55af5ea9c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phases(week):\n",
    "    if week < 0:\n",
    "        ph = 0\n",
    "    elif (week >= 0 and week <= 4):\n",
    "        ph = 1\n",
    "    elif (week >= 5 and week <= 12):\n",
    "        ph = 2\n",
    "    elif week > 12:\n",
    "        ph = 3\n",
    "    return ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3174191d-2625-4feb-ab65-ba2206b4f37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_u = np.load('uid_per_shb_fatigue.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cda2e352-7f53-42ca-a782-a4d6892b51fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_u2=np.load('uid_per_shb_fatigue(1).npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "724038dd-9079-4442-a9ca-186d3be981f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[325122, 365358, 496988, 547243, 1142579, 1214453, 1241286]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in lc_u if i not in lc_u2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b80c0fc7-e39c-402d-8b36-5beddf5ba024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[361648,\n",
       " 630454,\n",
       " 1000783,\n",
       " 1091037,\n",
       " 1110229,\n",
       " 1148848,\n",
       " 1149371,\n",
       " 1150130,\n",
       " 1219200,\n",
       " 1232440]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in lc_u2 if i not in lc_u]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a164dfdb-86bd-425b-84df-f46bdbd3af8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_test = pd.read_csv('pos_testdate.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3b5f142-994b-467b-83ff-05cf9ef6518b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([997743])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 996677 gibts nicht\n",
    "# 1216630 gibts auch nicht "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaed769c-9921-427f-babc-4d78a218fcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/datenspende-science/datenspende/utils/load_from_postgres.py:45: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(query, conn)\n",
      "/home/jovyan/datenspende-science/datenspende/utils/load_from_postgres.py:45: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/datenspende-science/datenspende/utils/load_from_postgres.py:45: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(query, conn)\n",
      "/home/jovyan/datenspende-science/datenspende/utils/load_from_postgres.py:45: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "630454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/datenspende-science/datenspende/utils/load_from_postgres.py:45: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(query, conn)\n",
      "/home/jovyan/datenspende-science/datenspende/utils/load_from_postgres.py:45: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/datenspende-science/datenspende/utils/load_from_postgres.py:45: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(query, conn)\n",
      "/home/jovyan/datenspende-science/datenspende/utils/load_from_postgres.py:45: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1091037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/datenspende-science/datenspende/utils/load_from_postgres.py:45: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(query, conn)\n",
      "/home/jovyan/datenspende-science/datenspende/utils/load_from_postgres.py:45: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1110229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/datenspende-science/datenspende/utils/load_from_postgres.py:45: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(query, conn)\n",
      "/home/jovyan/datenspende-science/datenspende/utils/load_from_postgres.py:45: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1148848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/datenspende-science/datenspende/utils/load_from_postgres.py:45: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(query, conn)\n",
      "/home/jovyan/datenspende-science/datenspende/utils/load_from_postgres.py:45: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1149371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/datenspende-science/datenspende/utils/load_from_postgres.py:45: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(query, conn)\n",
      "/home/jovyan/datenspende-science/datenspende/utils/load_from_postgres.py:45: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1150130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/datenspende-science/datenspende/utils/load_from_postgres.py:45: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(query, conn)\n",
      "/home/jovyan/datenspende-science/datenspende/utils/load_from_postgres.py:45: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1219200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/datenspende-science/datenspende/utils/load_from_postgres.py:45: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(query, conn)\n",
      "/home/jovyan/datenspende-science/datenspende/utils/load_from_postgres.py:45: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql(query, conn)\n"
     ]
    }
   ],
   "source": [
    "udf = get_epoch([lc_u[0]])\n",
    "udf = udf.rename(columns={\"customer\": \"user_id\"})\n",
    "udf = modify_df(udf)\n",
    "time_bins = sorted(udf['Time Bin 1'].unique())\n",
    "\n",
    "for us_id in [i for i in lc_u2 if i not in lc_u]:\n",
    "    gi = get_info([us_id])\n",
    "    if len(gi) > 0:\n",
    "        sex = 'female' if gi['element'].values[0] == 773 else 'male'\n",
    "        age = get_demo([us_id])['age'].values[0]\n",
    "\n",
    "        udf = get_epoch([us_id])\n",
    "        udf = udf.rename(columns={\"customer\": \"user_id\"})\n",
    "        udf = modify_df(udf)\n",
    "        #time_bins = sorted(udf['Time Bin 1'].unique())\n",
    "\n",
    "        df_comb = get_df(udf)\n",
    "\n",
    "        df_comb['day_of_week'] = pd.to_datetime(df_comb['date']).dt.dayofweek\n",
    "        df_comb['weekend'] = df_comb['day_of_week'].apply(lambda x: True if x >= 5 else False)\n",
    "\n",
    "        df_comb['dt'] = pd.to_datetime(pos_test['dt'][pos_test['user_id'].isin([us_id])].iloc[0])\n",
    "        td = pd.to_datetime(df_comb['date']) - df_comb['dt'] \n",
    "        df_comb['week_totest'] = td.apply(lambda x: -(x.days// - 7))\n",
    "\n",
    "        df_comb['time'] = df_comb['Time Bin 1'].map(dict(zip(list(df_comb['Time Bin 1'].unique()),list(range(97)))))\n",
    "        df_comb['phase'] = df_comb['week_totest'].apply(lambda x: phases(x))\n",
    "\n",
    "        if sex == 'female':\n",
    "            MAX_HR = 206 - (0.88 * age)\n",
    "        else:\n",
    "            MAX_HR = 208 - (0.7 * age)\n",
    "\n",
    "        df_comb['d to max hr [%]'] = (df_comb['hr'])/ MAX_HR * 100\n",
    "\n",
    "        df_comb.to_csv('user_df/'+sex+str(age)+str(us_id)+'.csv')\n",
    "    print(us_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca1ffa6a-1ee3-45ef-842c-96f1511518ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1216630"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "374cdbfb-ec9f-4479-abad-2ed698a669f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: dt, dtype: object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_test['dt'][pos_test['user_id'].isin([us_id])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ada4ea21-35bb-407e-bf51-4a8367e0ccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "    dates = []\n",
    "    tb = []\n",
    "\n",
    "    for d in udf['date'].unique():\n",
    "        for t in time_bins:\n",
    "            dates.append(d)\n",
    "            tb.append(t)\n",
    "            \n",
    "    df_comb = pd.DataFrame(data={'date':dates ,'Time Bin 1':tb})\n",
    "    \n",
    "    for v in [1000, 3000, 2000]:\n",
    "        if len(udf[udf['type'] == v]) > 0:\n",
    "            vt_i = udf[udf['type'] == v]['valueType'].iloc[0]\n",
    "            if vt_i == 0:\n",
    "                vt = 'doubleValue'\n",
    "            elif vt_i == 1:\n",
    "                vt = 'longValue'\n",
    "            elif vt_i == 2:\n",
    "                vt = 'booleanValue'\n",
    "\n",
    "            u_val_df = udf[udf['type'] == v][[vt, 'date', 'code', 'Time Bin 1', 'Time Bin 2', 'Interval','startTimestamp', 'endTimestamp']]\n",
    "            val_code = u_val_df['code'].iloc[0]\n",
    "\n",
    "            pdt_df = []\n",
    "            for date in u_val_df['date'].unique():\n",
    "                pdt_df.append(combine_overlaps(u_val_df[u_val_df['date']==date],vt)) \n",
    "            dn = pd.concat(pdt_df, axis=0)\n",
    "            # re-index\n",
    "            dn.index = np.arange(1, len(dn) + 1)\n",
    "\n",
    "\n",
    "            if vt_i == 0:\n",
    "                i_multiple_bins = dn[dn['Time Bin 1']!= dn['Time Bin 2']].index\n",
    "                dn = multiple_bins(i_multiple_bins, dn, time_bins, bin_s, bin_e,vt) \n",
    "                dng = dn[['date', 'Time Bin 1',vt]].groupby(['date', 'Time Bin 1']).sum( ).reset_index(level=[0,1])\n",
    "                dng = dng.rename(columns={vt:'steps'})\n",
    "\n",
    "                df_comb = pd.merge(df_comb,dng, how='outer', on=['date', 'Time Bin 1'])\n",
    "\n",
    "            elif vt_i == 1:\n",
    "\n",
    "                dn['s'] = dn['Interval'].apply(lambda x: x.length.total_seconds())\n",
    "                i_multiple_bins = dn[dn['Time Bin 1']!= dn['Time Bin 2']].index\n",
    "                i_multiple_bins = dn[(dn['Time Bin 1']!= dn['Time Bin 2'])&(dn['s'] != 60)].index\n",
    "                dn = multiple_bins(i_multiple_bins, dn, time_bins, bin_s, bin_e,vt) \n",
    "\n",
    "                dng = dn[['date', 'Time Bin 1',vt]].groupby(['date', 'Time Bin 1']).mean( ).reset_index(level=[0,1])\n",
    "                dng = dng.rename(columns = {vt:'hr'})         \n",
    "\n",
    "                #merge w df above\n",
    "                df_comb = pd.merge(df_comb, dng, how='outer', on=['date', 'Time Bin 1'])\n",
    "\n",
    "            elif vt_i == 2:\n",
    "                i_multiple_bins = dn[(dn['Time Bin 1']!= dn['Time Bin 2'])].index\n",
    "                dn = multiple_bins(i_multiple_bins, dn, time_bins, bin_s, bin_e,vt) \n",
    "                dng = dn[['date', 'Time Bin 1',vt]].groupby(['date', 'Time Bin 1']).mean( ).reset_index(level=[0,1])\n",
    "                dng = dng.rename(columns = {vt:'sleep'})    \n",
    "                #merge w df above\n",
    "                df_comb = pd.merge(df_comb, dng, how='outer', on=['date', 'Time Bin 1'])\n",
    "        elif len(udf[udf['type'] == v]) == 0:\n",
    "            \n",
    "            if v == 1000:\n",
    "                df_comb['steps'] = np.nan\n",
    "            elif v == 3000:\n",
    "                df_comb['hr'] = np.nan\n",
    "            elif v == 2000:\n",
    "                df_comb['sleep'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3cc336e-2176-485a-b37e-3553dd2ee77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51633ec2-6c07-431f-aca6-b473116995eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,10))\n",
    "df = dfs[1]\n",
    "w = -10\n",
    "plt.plot(range(len(df[df['week_totest']==w])),df[df['week_totest']==w]['steps'].values)\n",
    "plt.xticks(range(len(df[df['week_totest']==w])),df[df['week_totest']==w]['Time Bin 1'],rotation\n",
    "           =90);\n",
    "w = 0\n",
    "plt.plot(range(len(df[df['week_totest']==w])),df[df['week_totest']==w]['steps'].values)\n",
    "plt.xticks(range(len(df[df['week_totest']==w])),df[df['week_totest']==w]['Time Bin 1'],rotation\n",
    "           =90);\n",
    "\n",
    "w = 10\n",
    "plt.plot(range(len(df[df['week_totest']==w])),df[df['week_totest']==w]['steps'].values)\n",
    "plt.xticks(range(len(df[df['week_totest']==w])),df[df['week_totest']==w]['Time Bin 1'],rotation\n",
    "           =90);\n",
    "\n",
    "plt.legend(['-10 w', '0 w', '10 w']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f17cd57-00bd-4fbc-981b-92e889b70ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,10))\n",
    "df = dfs[1]\n",
    "w = -10\n",
    "plt.plot(range(len(df[df['week_totest']==w])),df[df['week_totest']==w]['hr'].values)\n",
    "plt.xticks(range(len(df[df['week_totest']==w])),df[df['week_totest']==w]['Time Bin 1'],rotation\n",
    "           =90);\n",
    "w = 0\n",
    "plt.plot(range(len(df[df['week_totest']==w])),df[df['week_totest']==w]['hr'].values)\n",
    "plt.xticks(range(len(df[df['week_totest']==w])),df[df['week_totest']==w]['Time Bin 1'],rotation\n",
    "           =90);\n",
    "\n",
    "w = 10\n",
    "plt.plot(range(len(df[df['week_totest']==w])),df[df['week_totest']==w]['hr'].values)\n",
    "plt.xticks(range(len(df[df['week_totest']==w])),df[df['week_totest']==w]['Time Bin 1'],rotation\n",
    "           =90);\n",
    "\n",
    "plt.legend(['-10 w', '0 w', '10 w']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7c0b3a-de62-44bb-a34d-eaf381a6c436",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d753a8-6e7a-402c-8534-69834bf8e114",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2, figsize=(16,8))\n",
    "u = ['negative user', 'persistent symtpoms user']\n",
    "for i in range(2):\n",
    "    for ph in [0,1,2,3]:\n",
    "        df = dfs[i]\n",
    "        dfph = df[df['phase'] == ph]\n",
    "        dfph_we = dfph[dfph['weekend'] == True]\n",
    "        dfph_wd = dfph[dfph['weekend'] == False]\n",
    "        ax[0,i].errorbar(dfph_we.groupby('time').mean().index, dfph_we.groupby('time').mean()['hr'], yerr=0)\n",
    "        ax[1,i].errorbar(dfph_wd.groupby('time').mean().index, dfph_wd.groupby('time').mean()['hr'], yerr=0)\n",
    "        ax[0,i].set_ylabel('heart rate weekend')\n",
    "        ax[1,i].set_ylabel('heart rate week')\n",
    "        ax[1,i].set_xlabel('time bins')\n",
    "        ax[0,i].set_title(u[i])\n",
    "plt.legend(['pre', 'acute', 'sub-acute', 'post']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b914b81-5a0b-4673-a98e-65bf02367c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(16,8))\n",
    "u = ['negative user', 'persistent symtpoms user']\n",
    "for i in range(2):\n",
    "    for ph in [0,1,2,3]:\n",
    "        df = dfs[i]\n",
    "        dfph = df[df['phase'] == ph]\n",
    "        ax[i].errorbar(dfph.groupby('time').mean().index, dfph.groupby('time').mean()['rhr'], yerr=dfph.groupby('time').std()['rhr'])\n",
    "        ax[i].set_title(u[i])\n",
    "plt.legend(['pre', 'acute', 'sub-acute', 'post']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36c0dfe-e74c-46c0-86a0-b9b75c23dd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(16,8))\n",
    "u = ['negative user', 'persistent symtpoms user']\n",
    "for i in range(2):\n",
    "    for ph in [0,1,2,3]:\n",
    "        df = dfs[i]\n",
    "        dfph = df[df['phase'] == ph]\n",
    "        ax[i].errorbar(dfph.groupby('time').mean().index, dfph.groupby('time').mean()['steps_per_s'], yerr=dfph.groupby('time').std()['steps_per_s'])\n",
    "        ax[i].set_title(u[i])\n",
    "plt.legend(['pre', 'acute', 'sub-acute', 'post']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2063d664-4cad-43cc-b232-cb5af350edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,4, figsize=(16,8))\n",
    "phases = ['pre', 'acute', 'sub-acute', 'post']\n",
    "for ph in [0,1,2,3]:\n",
    "    df = dfs[1]\n",
    "    dfph = df[df['phase'] == ph]\n",
    "    ax[ph].hist(dfph[dfph['sleep'] != 1]['hr'], density = True, histtype='step')\n",
    "    ax[ph].hist(dfph[dfph['sleep'] == 1]['hr'], density = True, histtype='step')\n",
    "    ax[ph].set_title(phases[ph])\n",
    "plt.legend(['day', 'night']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b868cbee-56da-428a-9691-79e975e83193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (epoch_kl)",
   "language": "python",
   "name": "epoch_kl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
